{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4540a450",
   "metadata": {},
   "source": [
    "# <center style='color:deeppink'>Calculate FID (`Frechet Inception Distance`) using PyTorch</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c0a24",
   "metadata": {},
   "source": [
    "# 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60d94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE PYTHON 3.11.1 KERNEL ON YOUR LAPTOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebac148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cu121\n",
      "Torchvision version: 0.16.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch version:', torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "print('Torchvision version:', torchvision.__version__)\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom module\n",
    "from INCEPTION import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39cafd",
   "metadata": {},
   "source": [
    "# 2. Define `ImagePathDataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2a2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXTENSIONS = {'jpg', 'png'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df4450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePathDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        \n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9961d",
   "metadata": {},
   "source": [
    "# 3. Define `get_activations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097823ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(files, model, batch_size, dims, device='cpu'):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if batch_size > len(files):\n",
    "        batch_size = len(files)\n",
    "        \n",
    "    dataset = ImagePathDataset(files, transform=transforms.ToTensor())\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    pred_arr = np.empty((len(files), dims))\n",
    "    start_idx = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            pred = model(batch)[0]\n",
    "            \n",
    "        if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "            pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "        \n",
    "        pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "        pred_arr[start_idx:start_idx+pred.shape[0]] = pred\n",
    "        start_idx = start_idx + pred.shape[0]\n",
    "        \n",
    "    return pred_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf2939",
   "metadata": {},
   "source": [
    "# 4. Define `calculate_frechet_distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22637133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_distance(mu1, mu2, sigma1, sigma2, eps=1e-6):\n",
    "    \n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "    \n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "    \n",
    "    assert mu1.shape == mu2.shape, 'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, 'Training and test covariances have different dimensions'\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "\n",
    "    \n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces sigular product; adding %s to diagonal cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "        \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        # covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "    \n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d63eb7",
   "metadata": {},
   "source": [
    "# 5. Define `calculate_activation_statistics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50baba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activation_statistics(files, model, batch_size, dims, device='cpu'):\n",
    "    \n",
    "    act = get_activations(files, model, batch_size, dims, device)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    \n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250504c",
   "metadata": {},
   "source": [
    "# 6. Define `compute_statistics_of_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b953a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics_of_path(path, model, batch_size, dims, device='cpu'):\n",
    "    \n",
    "    path = pathlib.Path(path)\n",
    "    files = sorted([file for ext in IMAGE_EXTENSIONS for file in path.glob('*.{}'.format(ext))])\n",
    "    mu, sigma = calculate_activation_statistics(files, model, batch_size, dims, device)\n",
    "    print(mu, sigma)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fc6f1",
   "metadata": {},
   "source": [
    "# 7. Calculate `FID distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41af0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid_given_paths(path1, path2, batch_size, dims, device='cpu'):\n",
    "    \n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "    model = InceptionV3([block_idx]).to(device)\n",
    "    \n",
    "    mu1, sigma1 = compute_statistics_of_path(path1, model, batch_size, dims, device)\n",
    "    mu2, sigma2 = compute_statistics_of_path(path2, model, batch_size, dims, device)\n",
    "    \n",
    "    fid_value = calculate_frechet_distance(mu1, mu2, sigma1, sigma2)\n",
    "    return print('FID distance:', round(fid_value, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8472971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in src_path: 2048\n",
      "Total images in gen_path: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25195728 1.39560402 0.28548145 ... 0.31830803 0.19201821 0.07183731] [[ 0.03106835  0.03885963 -0.00496966 ... -0.00912972  0.00093711\n",
      "  -0.00568965]\n",
      " [ 0.03885963  0.692958    0.07642591 ... -0.1108432  -0.01271496\n",
      "  -0.06193802]\n",
      " [-0.00496966  0.07642591  0.04195061 ... -0.01246667 -0.00412886\n",
      "  -0.00761096]\n",
      " ...\n",
      " [-0.00912972 -0.1108432  -0.01246667 ...  0.07171613  0.00386819\n",
      "   0.01036561]\n",
      " [ 0.00093711 -0.01271496 -0.00412886 ...  0.00386819  0.01916204\n",
      "  -0.00168936]\n",
      " [-0.00568965 -0.06193802 -0.00761096 ...  0.01036561 -0.00168936\n",
      "   0.01858369]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:26<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40462448 1.15166937 0.41913672 ... 0.36232411 0.17979012 0.08804709] [[ 0.05926778  0.01973837 -0.00040784 ... -0.00521519  0.00117018\n",
      "  -0.00111234]\n",
      " [ 0.01973837  0.36406866  0.03423299 ... -0.03154953 -0.00218196\n",
      "  -0.01038605]\n",
      " [-0.00040784  0.03423299  0.0816804  ... -0.02244934 -0.00115603\n",
      "  -0.00214513]\n",
      " ...\n",
      " [-0.00521519 -0.03154953 -0.02244934 ...  0.06557725  0.00683277\n",
      "   0.00371436]\n",
      " [ 0.00117018 -0.00218196 -0.00115603 ...  0.00683277  0.0257325\n",
      "   0.00123318]\n",
      " [-0.00111234 -0.01038605 -0.00214513 ...  0.00371436  0.00123318\n",
      "   0.01371712]]\n",
      "FID distance: (189.506-0j)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panth\\AppData\\Local\\Temp\\ipykernel_25220\\9098946.py:10: DeprecationWarning: The Python built-in `round` is deprecated for complex scalars, and will raise a `TypeError` in a future release. Use `np.round` or `scalar.round` instead.\n",
      "  return print('FID distance:', round(fid_value, 3))\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 64\n",
    "dims = 2048\n",
    "\n",
    "src_path = os.getcwd() + '/splitAugust3299'\n",
    "gen_path = os.getcwd() + '/src299'\n",
    "\n",
    "print('Total images in src_path:', len(next(os.walk('splitAugust3299'))[2]))\n",
    "print('Total images in gen_path:', len(next(os.walk('src299'))[2]))\n",
    "\n",
    "calculate_fid_given_paths(path1=src_path, path2=gen_path, batch_size=batch_size, dims=dims, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a053ea0",
   "metadata": {},
   "source": [
    "# A lower value of `FID distance` denotes more similarity with source images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
