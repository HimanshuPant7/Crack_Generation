{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimanshuPant7/Crack_Generation/blob/main/WGAN_Crack_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpzXdgi12QR5",
        "outputId": "cf29bedb-e4f4-4b41-9a2d-0e18a9dd58f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aRBmOCT2Z1-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCRLU1EK6d9u"
      },
      "outputs": [],
      "source": [
        "class CrackDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.images[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        image = image.resize((256, 256), resample=Image.BICUBIC)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSpPlwsQ8-_Z"
      },
      "source": [
        "DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dlFrSzH8-gV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLKLCJDW2fmE"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axIOyQN4Cffk"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (512) x 4 x 4\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (256) x 8 x 8\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (128) x 16 x 16\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (64) x 32 x 32\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (32) x 64 x 64\n",
        "            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (16) x 128 x 128\n",
        "            nn.ConvTranspose2d(16, 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (8) x 256 x 256\n",
        "            nn.ConvTranspose2d(8, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (3) x 256 x 256\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo-wbCYt2hqW"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 4, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 4, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 4, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.fc1 = nn.Linear(512 * 4 * 4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = nn.functional.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
        "        x = nn.functional.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
        "        x = nn.functional.leaky_relu(self.bn4(self.conv4(x)), 0.2)\n",
        "        x = x.view(-1, 512 * 4 * 4)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCuwy3zE3OK8"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm\") != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find(\"Linear\") != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KbTlGUA3TvE",
        "outputId": "efcfd722-c8d7-47e8-e472-cddb1aedd07c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=8192, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "G = Generator()\n",
        "D = Discriminator()\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KcOtVRs3UYv"
      },
      "outputs": [],
      "source": [
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "G_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])\n",
        "D_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGIPMkbP4OU_",
        "outputId": "d8d6a941-1bc9-4c98-e8c7-2cf0558bff24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=8192, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the models to the device\n",
        "G.to(device)\n",
        "D.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9Si-Gj65-bk",
        "outputId": "e20bc3a0-2059-40e9-f588-7e63dc2d3d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/2000], Batch [0/4], D_loss: -0.0323, G_loss: 0.0011\n",
            "Epoch [1/2000], Batch [0/4], D_loss: -0.1694, G_loss: -0.0007\n",
            "Epoch [2/2000], Batch [0/4], D_loss: -0.3520, G_loss: 0.0060\n",
            "Epoch [3/2000], Batch [0/4], D_loss: -0.4452, G_loss: 0.0169\n",
            "Epoch [4/2000], Batch [0/4], D_loss: -0.5097, G_loss: 0.0298\n",
            "Epoch [5/2000], Batch [0/4], D_loss: -0.5595, G_loss: 0.0433\n",
            "Epoch [6/2000], Batch [0/4], D_loss: -0.6024, G_loss: 0.0572\n",
            "Epoch [7/2000], Batch [0/4], D_loss: -0.6384, G_loss: 0.0709\n",
            "Epoch [8/2000], Batch [0/4], D_loss: -0.6691, G_loss: 0.0846\n",
            "Epoch [9/2000], Batch [0/4], D_loss: -0.6961, G_loss: 0.0986\n",
            "Epoch [10/2000], Batch [0/4], D_loss: -0.7205, G_loss: 0.1122\n",
            "Epoch [11/2000], Batch [0/4], D_loss: -0.7437, G_loss: 0.1262\n",
            "Epoch [12/2000], Batch [0/4], D_loss: -0.7666, G_loss: 0.1413\n",
            "Epoch [13/2000], Batch [0/4], D_loss: -0.7875, G_loss: 0.1574\n",
            "Epoch [14/2000], Batch [0/4], D_loss: -0.8077, G_loss: 0.1748\n",
            "Epoch [15/2000], Batch [0/4], D_loss: -0.8300, G_loss: 0.1923\n",
            "Epoch [16/2000], Batch [0/4], D_loss: -0.8507, G_loss: 0.2089\n",
            "Epoch [17/2000], Batch [0/4], D_loss: -0.8708, G_loss: 0.2232\n",
            "Epoch [18/2000], Batch [0/4], D_loss: -0.8842, G_loss: 0.2323\n",
            "Epoch [19/2000], Batch [0/4], D_loss: -0.8949, G_loss: 0.2394\n",
            "Epoch [20/2000], Batch [0/4], D_loss: -0.9045, G_loss: 0.2444\n",
            "Epoch [21/2000], Batch [0/4], D_loss: -0.9114, G_loss: 0.2494\n",
            "Epoch [22/2000], Batch [0/4], D_loss: -0.9176, G_loss: 0.2520\n",
            "Epoch [23/2000], Batch [0/4], D_loss: -0.9234, G_loss: 0.2553\n",
            "Epoch [24/2000], Batch [0/4], D_loss: -0.9289, G_loss: 0.2580\n",
            "Epoch [25/2000], Batch [0/4], D_loss: -0.9326, G_loss: 0.2598\n",
            "Epoch [26/2000], Batch [0/4], D_loss: -0.9365, G_loss: 0.2616\n",
            "Epoch [27/2000], Batch [0/4], D_loss: -0.9400, G_loss: 0.2637\n",
            "Epoch [28/2000], Batch [0/4], D_loss: -0.9429, G_loss: 0.2654\n",
            "Epoch [29/2000], Batch [0/4], D_loss: -0.9454, G_loss: 0.2671\n",
            "Epoch [30/2000], Batch [0/4], D_loss: -0.9482, G_loss: 0.2688\n",
            "Epoch [31/2000], Batch [0/4], D_loss: -0.9496, G_loss: 0.2690\n",
            "Epoch [32/2000], Batch [0/4], D_loss: -0.9515, G_loss: 0.2709\n",
            "Epoch [33/2000], Batch [0/4], D_loss: -0.9530, G_loss: 0.2722\n",
            "Epoch [34/2000], Batch [0/4], D_loss: -0.9548, G_loss: 0.2734\n",
            "Epoch [35/2000], Batch [0/4], D_loss: -0.9562, G_loss: 0.2746\n",
            "Epoch [36/2000], Batch [0/4], D_loss: -0.9583, G_loss: 0.2761\n",
            "Epoch [37/2000], Batch [0/4], D_loss: -0.9589, G_loss: 0.2766\n",
            "Epoch [38/2000], Batch [0/4], D_loss: -0.9607, G_loss: 0.2780\n",
            "Epoch [39/2000], Batch [0/4], D_loss: -0.9618, G_loss: 0.2792\n",
            "Epoch [40/2000], Batch [0/4], D_loss: -0.9596, G_loss: 0.2799\n",
            "Epoch [41/2000], Batch [0/4], D_loss: -0.9632, G_loss: 0.2812\n",
            "Epoch [42/2000], Batch [0/4], D_loss: -0.9658, G_loss: 0.2820\n",
            "Epoch [43/2000], Batch [0/4], D_loss: -0.9662, G_loss: 0.2825\n",
            "Epoch [44/2000], Batch [0/4], D_loss: -0.9665, G_loss: 0.2823\n",
            "Epoch [45/2000], Batch [0/4], D_loss: -0.9649, G_loss: 0.2816\n",
            "Epoch [46/2000], Batch [0/4], D_loss: -0.9676, G_loss: 0.2826\n",
            "Epoch [47/2000], Batch [0/4], D_loss: -0.9679, G_loss: 0.2828\n",
            "Epoch [48/2000], Batch [0/4], D_loss: -0.9668, G_loss: 0.2827\n",
            "Epoch [49/2000], Batch [0/4], D_loss: -0.9687, G_loss: 0.2831\n",
            "Epoch [50/2000], Batch [0/4], D_loss: -0.9694, G_loss: 0.2833\n",
            "Epoch [51/2000], Batch [0/4], D_loss: -0.9698, G_loss: 0.2835\n",
            "Epoch [52/2000], Batch [0/4], D_loss: -0.9699, G_loss: 0.2835\n",
            "Epoch [53/2000], Batch [0/4], D_loss: -0.9705, G_loss: 0.2837\n",
            "Epoch [54/2000], Batch [0/4], D_loss: -0.9704, G_loss: 0.2840\n",
            "Epoch [55/2000], Batch [0/4], D_loss: -0.9703, G_loss: 0.2838\n",
            "Epoch [56/2000], Batch [0/4], D_loss: -0.9715, G_loss: 0.2843\n",
            "Epoch [57/2000], Batch [0/4], D_loss: -0.9670, G_loss: 0.2839\n",
            "Epoch [58/2000], Batch [0/4], D_loss: -0.9715, G_loss: 0.2845\n",
            "Epoch [59/2000], Batch [0/4], D_loss: -0.9727, G_loss: 0.2850\n",
            "Epoch [60/2000], Batch [0/4], D_loss: -0.9729, G_loss: 0.2851\n",
            "Epoch [61/2000], Batch [0/4], D_loss: -0.9727, G_loss: 0.2847\n",
            "Epoch [62/2000], Batch [0/4], D_loss: -0.9739, G_loss: 0.2855\n",
            "Epoch [63/2000], Batch [0/4], D_loss: -0.9737, G_loss: 0.2856\n",
            "Epoch [64/2000], Batch [0/4], D_loss: -0.9740, G_loss: 0.2856\n",
            "Epoch [65/2000], Batch [0/4], D_loss: -0.9740, G_loss: 0.2849\n",
            "Epoch [66/2000], Batch [0/4], D_loss: -0.9748, G_loss: 0.2860\n",
            "Epoch [67/2000], Batch [0/4], D_loss: -0.9745, G_loss: 0.2859\n",
            "Epoch [68/2000], Batch [0/4], D_loss: -0.9747, G_loss: 0.2859\n",
            "Epoch [69/2000], Batch [0/4], D_loss: -0.9561, G_loss: 0.2772\n",
            "Epoch [70/2000], Batch [0/4], D_loss: -0.9720, G_loss: 0.2854\n",
            "Epoch [71/2000], Batch [0/4], D_loss: -0.9753, G_loss: 0.2866\n",
            "Epoch [72/2000], Batch [0/4], D_loss: -0.9755, G_loss: 0.2866\n",
            "Epoch [73/2000], Batch [0/4], D_loss: -0.9761, G_loss: 0.2868\n",
            "Epoch [74/2000], Batch [0/4], D_loss: -0.9760, G_loss: 0.2864\n",
            "Epoch [75/2000], Batch [0/4], D_loss: -0.9762, G_loss: 0.2867\n",
            "Epoch [76/2000], Batch [0/4], D_loss: -0.9760, G_loss: 0.2863\n",
            "Epoch [77/2000], Batch [0/4], D_loss: -0.9770, G_loss: 0.2868\n",
            "Epoch [78/2000], Batch [0/4], D_loss: -0.9768, G_loss: 0.2866\n",
            "Epoch [79/2000], Batch [0/4], D_loss: -0.9767, G_loss: 0.2865\n",
            "Epoch [80/2000], Batch [0/4], D_loss: -0.9764, G_loss: 0.2866\n",
            "Epoch [81/2000], Batch [0/4], D_loss: -0.9773, G_loss: 0.2866\n",
            "Epoch [82/2000], Batch [0/4], D_loss: -0.9778, G_loss: 0.2868\n",
            "Epoch [83/2000], Batch [0/4], D_loss: -0.9777, G_loss: 0.2868\n",
            "Epoch [84/2000], Batch [0/4], D_loss: -0.9777, G_loss: 0.2868\n",
            "Epoch [85/2000], Batch [0/4], D_loss: -0.9780, G_loss: 0.2868\n",
            "Epoch [86/2000], Batch [0/4], D_loss: -0.9781, G_loss: 0.2869\n",
            "Epoch [87/2000], Batch [0/4], D_loss: -0.9784, G_loss: 0.2868\n",
            "Epoch [88/2000], Batch [0/4], D_loss: -0.9785, G_loss: 0.2870\n",
            "Epoch [89/2000], Batch [0/4], D_loss: -0.9784, G_loss: 0.2869\n",
            "Epoch [90/2000], Batch [0/4], D_loss: -0.9771, G_loss: 0.2852\n",
            "Epoch [91/2000], Batch [0/4], D_loss: -0.9716, G_loss: 0.2832\n",
            "Epoch [92/2000], Batch [0/4], D_loss: -0.9774, G_loss: 0.2870\n",
            "Epoch [93/2000], Batch [0/4], D_loss: -0.9784, G_loss: 0.2874\n",
            "Epoch [94/2000], Batch [0/4], D_loss: -0.9793, G_loss: 0.2877\n",
            "Epoch [95/2000], Batch [0/4], D_loss: -0.9773, G_loss: 0.2869\n",
            "Epoch [96/2000], Batch [0/4], D_loss: -0.9791, G_loss: 0.2874\n",
            "Epoch [97/2000], Batch [0/4], D_loss: -0.9792, G_loss: 0.2875\n",
            "Epoch [98/2000], Batch [0/4], D_loss: -0.9781, G_loss: 0.2870\n",
            "Epoch [99/2000], Batch [0/4], D_loss: -0.9796, G_loss: 0.2875\n",
            "Epoch [100/2000], Batch [0/4], D_loss: -0.9775, G_loss: 0.2866\n",
            "Epoch [101/2000], Batch [0/4], D_loss: -0.9795, G_loss: 0.2874\n",
            "Epoch [102/2000], Batch [0/4], D_loss: -0.9791, G_loss: 0.2866\n",
            "Epoch [103/2000], Batch [0/4], D_loss: -0.9800, G_loss: 0.2875\n",
            "Epoch [104/2000], Batch [0/4], D_loss: -0.9798, G_loss: 0.2872\n",
            "Epoch [105/2000], Batch [0/4], D_loss: -0.9792, G_loss: 0.2874\n",
            "Epoch [106/2000], Batch [0/4], D_loss: -0.9763, G_loss: 0.2848\n",
            "Epoch [107/2000], Batch [0/4], D_loss: -0.9789, G_loss: 0.2873\n",
            "Epoch [108/2000], Batch [0/4], D_loss: -0.9799, G_loss: 0.2877\n",
            "Epoch [109/2000], Batch [0/4], D_loss: -0.9796, G_loss: 0.2872\n",
            "Epoch [110/2000], Batch [0/4], D_loss: -0.9796, G_loss: 0.2876\n",
            "Epoch [111/2000], Batch [0/4], D_loss: -0.9792, G_loss: 0.2852\n",
            "Epoch [112/2000], Batch [0/4], D_loss: -0.9704, G_loss: 0.2805\n",
            "Epoch [113/2000], Batch [0/4], D_loss: -0.8864, G_loss: 0.2200\n",
            "Epoch [114/2000], Batch [0/4], D_loss: -0.9448, G_loss: 0.2717\n",
            "Epoch [115/2000], Batch [0/4], D_loss: -0.9619, G_loss: 0.2809\n",
            "Epoch [116/2000], Batch [0/4], D_loss: -0.9647, G_loss: 0.2817\n",
            "Epoch [117/2000], Batch [0/4], D_loss: -0.9663, G_loss: 0.2826\n",
            "Epoch [118/2000], Batch [0/4], D_loss: -0.9666, G_loss: 0.2830\n",
            "Epoch [119/2000], Batch [0/4], D_loss: -0.9683, G_loss: 0.2829\n",
            "Epoch [120/2000], Batch [0/4], D_loss: -0.9693, G_loss: 0.2833\n",
            "Epoch [121/2000], Batch [0/4], D_loss: -0.9710, G_loss: 0.2842\n",
            "Epoch [122/2000], Batch [0/4], D_loss: -0.9717, G_loss: 0.2846\n",
            "Epoch [123/2000], Batch [0/4], D_loss: -0.9717, G_loss: 0.2845\n",
            "Epoch [124/2000], Batch [0/4], D_loss: -0.9729, G_loss: 0.2845\n",
            "Epoch [125/2000], Batch [0/4], D_loss: -0.9729, G_loss: 0.2846\n",
            "Epoch [126/2000], Batch [0/4], D_loss: -0.9724, G_loss: 0.2843\n",
            "Epoch [127/2000], Batch [0/4], D_loss: -0.9733, G_loss: 0.2846\n",
            "Epoch [128/2000], Batch [0/4], D_loss: -0.9735, G_loss: 0.2841\n",
            "Epoch [129/2000], Batch [0/4], D_loss: -0.9727, G_loss: 0.2844\n",
            "Epoch [130/2000], Batch [0/4], D_loss: -0.9733, G_loss: 0.2848\n",
            "Epoch [131/2000], Batch [0/4], D_loss: -0.9742, G_loss: 0.2848\n",
            "Epoch [132/2000], Batch [0/4], D_loss: -0.9747, G_loss: 0.2851\n",
            "Epoch [133/2000], Batch [0/4], D_loss: -0.9751, G_loss: 0.2854\n",
            "Epoch [134/2000], Batch [0/4], D_loss: -0.9750, G_loss: 0.2851\n",
            "Epoch [135/2000], Batch [0/4], D_loss: -0.9751, G_loss: 0.2853\n",
            "Epoch [136/2000], Batch [0/4], D_loss: -0.9753, G_loss: 0.2850\n",
            "Epoch [137/2000], Batch [0/4], D_loss: -0.9762, G_loss: 0.2858\n",
            "Epoch [138/2000], Batch [0/4], D_loss: -0.9757, G_loss: 0.2857\n",
            "Epoch [139/2000], Batch [0/4], D_loss: -0.9752, G_loss: 0.2853\n",
            "Epoch [140/2000], Batch [0/4], D_loss: -0.9760, G_loss: 0.2857\n",
            "Epoch [141/2000], Batch [0/4], D_loss: -0.9757, G_loss: 0.2859\n",
            "Epoch [142/2000], Batch [0/4], D_loss: -0.9772, G_loss: 0.2863\n",
            "Epoch [143/2000], Batch [0/4], D_loss: -0.9774, G_loss: 0.2865\n",
            "Epoch [144/2000], Batch [0/4], D_loss: -0.9780, G_loss: 0.2867\n",
            "Epoch [145/2000], Batch [0/4], D_loss: -0.9772, G_loss: 0.2864\n",
            "Epoch [146/2000], Batch [0/4], D_loss: -0.9779, G_loss: 0.2868\n",
            "Epoch [147/2000], Batch [0/4], D_loss: -0.9772, G_loss: 0.2869\n",
            "Epoch [148/2000], Batch [0/4], D_loss: -0.9774, G_loss: 0.2866\n",
            "Epoch [149/2000], Batch [0/4], D_loss: -0.9773, G_loss: 0.2870\n",
            "Epoch [150/2000], Batch [0/4], D_loss: -0.9776, G_loss: 0.2873\n",
            "Epoch [151/2000], Batch [0/4], D_loss: -0.9781, G_loss: 0.2871\n",
            "Epoch [152/2000], Batch [0/4], D_loss: -0.9793, G_loss: 0.2876\n",
            "Epoch [153/2000], Batch [0/4], D_loss: -0.9794, G_loss: 0.2876\n",
            "Epoch [154/2000], Batch [0/4], D_loss: -0.9790, G_loss: 0.2877\n",
            "Epoch [155/2000], Batch [0/4], D_loss: -0.9794, G_loss: 0.2874\n",
            "Epoch [156/2000], Batch [0/4], D_loss: -0.9796, G_loss: 0.2876\n",
            "Epoch [157/2000], Batch [0/4], D_loss: -0.9801, G_loss: 0.2877\n",
            "Epoch [158/2000], Batch [0/4], D_loss: -0.9792, G_loss: 0.2872\n",
            "Epoch [159/2000], Batch [0/4], D_loss: -0.9794, G_loss: 0.2871\n",
            "Epoch [160/2000], Batch [0/4], D_loss: -0.9800, G_loss: 0.2875\n",
            "Epoch [161/2000], Batch [0/4], D_loss: -0.9801, G_loss: 0.2877\n",
            "Epoch [162/2000], Batch [0/4], D_loss: -0.9800, G_loss: 0.2873\n",
            "Epoch [163/2000], Batch [0/4], D_loss: -0.9803, G_loss: 0.2876\n",
            "Epoch [164/2000], Batch [0/4], D_loss: -0.9808, G_loss: 0.2876\n",
            "Epoch [165/2000], Batch [0/4], D_loss: -0.9804, G_loss: 0.2876\n",
            "Epoch [166/2000], Batch [0/4], D_loss: -0.9801, G_loss: 0.2871\n",
            "Epoch [167/2000], Batch [0/4], D_loss: -0.9803, G_loss: 0.2874\n",
            "Epoch [168/2000], Batch [0/4], D_loss: -0.9805, G_loss: 0.2875\n",
            "Epoch [169/2000], Batch [0/4], D_loss: -0.9797, G_loss: 0.2874\n",
            "Epoch [170/2000], Batch [0/4], D_loss: -0.9805, G_loss: 0.2875\n",
            "Epoch [171/2000], Batch [0/4], D_loss: -0.9807, G_loss: 0.2874\n",
            "Epoch [172/2000], Batch [0/4], D_loss: -0.9808, G_loss: 0.2875\n",
            "Epoch [173/2000], Batch [0/4], D_loss: -0.9809, G_loss: 0.2877\n",
            "Epoch [174/2000], Batch [0/4], D_loss: -0.9810, G_loss: 0.2878\n",
            "Epoch [175/2000], Batch [0/4], D_loss: -0.9808, G_loss: 0.2877\n",
            "Epoch [176/2000], Batch [0/4], D_loss: -0.9810, G_loss: 0.2877\n",
            "Epoch [177/2000], Batch [0/4], D_loss: -0.9811, G_loss: 0.2877\n",
            "Epoch [178/2000], Batch [0/4], D_loss: -0.9807, G_loss: 0.2870\n",
            "Epoch [179/2000], Batch [0/4], D_loss: -0.9813, G_loss: 0.2878\n",
            "Epoch [180/2000], Batch [0/4], D_loss: -0.9814, G_loss: 0.2879\n",
            "Epoch [181/2000], Batch [0/4], D_loss: -0.9809, G_loss: 0.2872\n",
            "Epoch [182/2000], Batch [0/4], D_loss: -0.9812, G_loss: 0.2878\n",
            "Epoch [183/2000], Batch [0/4], D_loss: -0.9816, G_loss: 0.2879\n",
            "Epoch [184/2000], Batch [0/4], D_loss: -0.9816, G_loss: 0.2880\n",
            "Epoch [185/2000], Batch [0/4], D_loss: -0.9811, G_loss: 0.2879\n",
            "Epoch [186/2000], Batch [0/4], D_loss: -0.9812, G_loss: 0.2878\n",
            "Epoch [187/2000], Batch [0/4], D_loss: -0.9814, G_loss: 0.2877\n",
            "Epoch [188/2000], Batch [0/4], D_loss: -0.9814, G_loss: 0.2876\n",
            "Epoch [189/2000], Batch [0/4], D_loss: -0.9814, G_loss: 0.2878\n",
            "Epoch [190/2000], Batch [0/4], D_loss: -0.9814, G_loss: 0.2880\n",
            "Epoch [191/2000], Batch [0/4], D_loss: -0.9815, G_loss: 0.2879\n",
            "Epoch [192/2000], Batch [0/4], D_loss: -0.9815, G_loss: 0.2878\n",
            "Epoch [193/2000], Batch [0/4], D_loss: -0.9810, G_loss: 0.2875\n",
            "Epoch [194/2000], Batch [0/4], D_loss: -0.9809, G_loss: 0.2876\n",
            "Epoch [195/2000], Batch [0/4], D_loss: -0.9808, G_loss: 0.2877\n",
            "Epoch [196/2000], Batch [0/4], D_loss: -0.9815, G_loss: 0.2876\n",
            "Epoch [197/2000], Batch [0/4], D_loss: -0.9814, G_loss: 0.2876\n",
            "Epoch [198/2000], Batch [0/4], D_loss: -0.9816, G_loss: 0.2876\n",
            "Epoch [199/2000], Batch [0/4], D_loss: -0.9815, G_loss: 0.2879\n",
            "Epoch [200/2000], Batch [0/4], D_loss: -0.9819, G_loss: 0.2879\n",
            "Epoch [201/2000], Batch [0/4], D_loss: -0.9818, G_loss: 0.2878\n",
            "Epoch [202/2000], Batch [0/4], D_loss: -0.9820, G_loss: 0.2882\n",
            "Epoch [203/2000], Batch [0/4], D_loss: -0.9815, G_loss: 0.2878\n",
            "Epoch [204/2000], Batch [0/4], D_loss: -0.9775, G_loss: 0.2851\n",
            "Epoch [205/2000], Batch [0/4], D_loss: -0.9816, G_loss: 0.2879\n",
            "Epoch [206/2000], Batch [0/4], D_loss: -0.9823, G_loss: 0.2882\n",
            "Epoch [207/2000], Batch [0/4], D_loss: -0.9821, G_loss: 0.2881\n",
            "Epoch [208/2000], Batch [0/4], D_loss: -0.9814, G_loss: 0.2878\n",
            "Epoch [209/2000], Batch [0/4], D_loss: -0.9825, G_loss: 0.2882\n",
            "Epoch [210/2000], Batch [0/4], D_loss: -0.9824, G_loss: 0.2882\n",
            "Epoch [211/2000], Batch [0/4], D_loss: -0.9823, G_loss: 0.2882\n",
            "Epoch [212/2000], Batch [0/4], D_loss: -0.9821, G_loss: 0.2881\n",
            "Epoch [213/2000], Batch [0/4], D_loss: -0.9821, G_loss: 0.2883\n",
            "Epoch [214/2000], Batch [0/4], D_loss: -0.9696, G_loss: 0.2831\n",
            "Epoch [215/2000], Batch [0/4], D_loss: -0.9811, G_loss: 0.2878\n",
            "Epoch [216/2000], Batch [0/4], D_loss: -0.9819, G_loss: 0.2881\n",
            "Epoch [217/2000], Batch [0/4], D_loss: -0.9820, G_loss: 0.2881\n",
            "Epoch [218/2000], Batch [0/4], D_loss: -0.9823, G_loss: 0.2883\n",
            "Epoch [219/2000], Batch [0/4], D_loss: -0.9822, G_loss: 0.2882\n",
            "Epoch [220/2000], Batch [0/4], D_loss: -0.9827, G_loss: 0.2884\n",
            "Epoch [221/2000], Batch [0/4], D_loss: -0.9824, G_loss: 0.2882\n",
            "Epoch [222/2000], Batch [0/4], D_loss: -0.9827, G_loss: 0.2884\n",
            "Epoch [223/2000], Batch [0/4], D_loss: -0.9826, G_loss: 0.2883\n",
            "Epoch [224/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2882\n",
            "Epoch [225/2000], Batch [0/4], D_loss: -0.9827, G_loss: 0.2885\n",
            "Epoch [226/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2885\n",
            "Epoch [227/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2885\n",
            "Epoch [228/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2885\n",
            "Epoch [229/2000], Batch [0/4], D_loss: -0.9826, G_loss: 0.2885\n",
            "Epoch [230/2000], Batch [0/4], D_loss: -0.9829, G_loss: 0.2885\n",
            "Epoch [231/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2884\n",
            "Epoch [232/2000], Batch [0/4], D_loss: -0.9827, G_loss: 0.2881\n",
            "Epoch [233/2000], Batch [0/4], D_loss: -0.9830, G_loss: 0.2886\n",
            "Epoch [234/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2888\n",
            "Epoch [235/2000], Batch [0/4], D_loss: -0.9831, G_loss: 0.2885\n",
            "Epoch [236/2000], Batch [0/4], D_loss: -0.9829, G_loss: 0.2885\n",
            "Epoch [237/2000], Batch [0/4], D_loss: -0.9830, G_loss: 0.2887\n",
            "Epoch [238/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2886\n",
            "Epoch [239/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2886\n",
            "Epoch [240/2000], Batch [0/4], D_loss: -0.9831, G_loss: 0.2886\n",
            "Epoch [241/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2885\n",
            "Epoch [242/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2887\n",
            "Epoch [243/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2887\n",
            "Epoch [244/2000], Batch [0/4], D_loss: -0.9831, G_loss: 0.2886\n",
            "Epoch [245/2000], Batch [0/4], D_loss: -0.9833, G_loss: 0.2887\n",
            "Epoch [246/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2887\n",
            "Epoch [247/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2888\n",
            "Epoch [248/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2887\n",
            "Epoch [249/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2888\n",
            "Epoch [250/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2882\n",
            "Epoch [251/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2887\n",
            "Epoch [252/2000], Batch [0/4], D_loss: -0.9835, G_loss: 0.2887\n",
            "Epoch [253/2000], Batch [0/4], D_loss: -0.9515, G_loss: 0.2546\n",
            "Epoch [254/2000], Batch [0/4], D_loss: -0.9791, G_loss: 0.2864\n",
            "Epoch [255/2000], Batch [0/4], D_loss: -0.9816, G_loss: 0.2879\n",
            "Epoch [256/2000], Batch [0/4], D_loss: -0.9822, G_loss: 0.2882\n",
            "Epoch [257/2000], Batch [0/4], D_loss: -0.9826, G_loss: 0.2882\n",
            "Epoch [258/2000], Batch [0/4], D_loss: -0.9826, G_loss: 0.2884\n",
            "Epoch [259/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2884\n",
            "Epoch [260/2000], Batch [0/4], D_loss: -0.9829, G_loss: 0.2885\n",
            "Epoch [261/2000], Batch [0/4], D_loss: -0.9825, G_loss: 0.2884\n",
            "Epoch [262/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2886\n",
            "Epoch [263/2000], Batch [0/4], D_loss: -0.9831, G_loss: 0.2886\n",
            "Epoch [264/2000], Batch [0/4], D_loss: -0.9833, G_loss: 0.2885\n",
            "Epoch [265/2000], Batch [0/4], D_loss: -0.9831, G_loss: 0.2884\n",
            "Epoch [266/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2886\n",
            "Epoch [267/2000], Batch [0/4], D_loss: -0.9833, G_loss: 0.2888\n",
            "Epoch [268/2000], Batch [0/4], D_loss: -0.9831, G_loss: 0.2887\n",
            "Epoch [269/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2887\n",
            "Epoch [270/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2887\n",
            "Epoch [271/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2888\n",
            "Epoch [272/2000], Batch [0/4], D_loss: -0.9835, G_loss: 0.2888\n",
            "Epoch [273/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2888\n",
            "Epoch [274/2000], Batch [0/4], D_loss: -0.9833, G_loss: 0.2887\n",
            "Epoch [275/2000], Batch [0/4], D_loss: -0.9837, G_loss: 0.2888\n",
            "Epoch [276/2000], Batch [0/4], D_loss: -0.9837, G_loss: 0.2889\n",
            "Epoch [277/2000], Batch [0/4], D_loss: -0.9837, G_loss: 0.2888\n",
            "Epoch [278/2000], Batch [0/4], D_loss: -0.9836, G_loss: 0.2889\n",
            "Epoch [279/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2889\n",
            "Epoch [280/2000], Batch [0/4], D_loss: -0.9837, G_loss: 0.2889\n",
            "Epoch [281/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2889\n",
            "Epoch [282/2000], Batch [0/4], D_loss: -0.9836, G_loss: 0.2886\n",
            "Epoch [283/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2888\n",
            "Epoch [284/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2890\n",
            "Epoch [285/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2890\n",
            "Epoch [286/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2890\n",
            "Epoch [287/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2888\n",
            "Epoch [288/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2890\n",
            "Epoch [289/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2889\n",
            "Epoch [290/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2888\n",
            "Epoch [291/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2890\n",
            "Epoch [292/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2890\n",
            "Epoch [293/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2890\n",
            "Epoch [294/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2889\n",
            "Epoch [295/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2891\n",
            "Epoch [296/2000], Batch [0/4], D_loss: -0.9837, G_loss: 0.2886\n",
            "Epoch [297/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2891\n",
            "Epoch [298/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2891\n",
            "Epoch [299/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2891\n",
            "Epoch [300/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2890\n",
            "Epoch [301/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2890\n",
            "Epoch [302/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2889\n",
            "Epoch [303/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2891\n",
            "Epoch [304/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2891\n",
            "Epoch [305/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2890\n",
            "Epoch [306/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2891\n",
            "Epoch [307/2000], Batch [0/4], D_loss: -0.9843, G_loss: 0.2891\n",
            "Epoch [308/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2891\n",
            "Epoch [309/2000], Batch [0/4], D_loss: -0.9823, G_loss: 0.2864\n",
            "Epoch [310/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2889\n",
            "Epoch [311/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2891\n",
            "Epoch [312/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2891\n",
            "Epoch [313/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2892\n",
            "Epoch [314/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2891\n",
            "Epoch [315/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2892\n",
            "Epoch [316/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2890\n",
            "Epoch [317/2000], Batch [0/4], D_loss: -0.9843, G_loss: 0.2892\n",
            "Epoch [318/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2892\n",
            "Epoch [319/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2892\n",
            "Epoch [320/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2891\n",
            "Epoch [321/2000], Batch [0/4], D_loss: -0.9697, G_loss: 0.2854\n",
            "Epoch [322/2000], Batch [0/4], D_loss: -0.9825, G_loss: 0.2883\n",
            "Epoch [323/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2889\n",
            "Epoch [324/2000], Batch [0/4], D_loss: -0.9835, G_loss: 0.2888\n",
            "Epoch [325/2000], Batch [0/4], D_loss: -0.9836, G_loss: 0.2889\n",
            "Epoch [326/2000], Batch [0/4], D_loss: -0.9835, G_loss: 0.2889\n",
            "Epoch [327/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2889\n",
            "Epoch [328/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2890\n",
            "Epoch [329/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2890\n",
            "Epoch [330/2000], Batch [0/4], D_loss: -0.9837, G_loss: 0.2888\n",
            "Epoch [331/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2890\n",
            "Epoch [332/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2891\n",
            "Epoch [333/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2890\n",
            "Epoch [334/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2890\n",
            "Epoch [335/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2891\n",
            "Epoch [336/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2891\n",
            "Epoch [337/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2891\n",
            "Epoch [338/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2891\n",
            "Epoch [339/2000], Batch [0/4], D_loss: -0.9843, G_loss: 0.2892\n",
            "Epoch [340/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2891\n",
            "Epoch [341/2000], Batch [0/4], D_loss: -0.9836, G_loss: 0.2888\n",
            "Epoch [342/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2891\n",
            "Epoch [343/2000], Batch [0/4], D_loss: -0.9843, G_loss: 0.2892\n",
            "Epoch [344/2000], Batch [0/4], D_loss: -0.9843, G_loss: 0.2892\n",
            "Epoch [345/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2893\n",
            "Epoch [346/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2892\n",
            "Epoch [347/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2890\n",
            "Epoch [348/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2892\n",
            "Epoch [349/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2893\n",
            "Epoch [350/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2893\n",
            "Epoch [351/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2892\n",
            "Epoch [352/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2891\n",
            "Epoch [353/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2892\n",
            "Epoch [354/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2892\n",
            "Epoch [355/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2893\n",
            "Epoch [356/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2892\n",
            "Epoch [357/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2890\n",
            "Epoch [358/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2893\n",
            "Epoch [359/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2893\n",
            "Epoch [360/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2891\n",
            "Epoch [361/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2892\n",
            "Epoch [362/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2892\n",
            "Epoch [363/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2892\n",
            "Epoch [364/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2892\n",
            "Epoch [365/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2887\n",
            "Epoch [366/2000], Batch [0/4], D_loss: -0.9843, G_loss: 0.2891\n",
            "Epoch [367/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2893\n",
            "Epoch [368/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2893\n",
            "Epoch [369/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2891\n",
            "Epoch [370/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2892\n",
            "Epoch [371/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2892\n",
            "Epoch [372/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2893\n",
            "Epoch [373/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2893\n",
            "Epoch [374/2000], Batch [0/4], D_loss: -0.9822, G_loss: 0.2884\n",
            "Epoch [375/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2891\n",
            "Epoch [376/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2892\n",
            "Epoch [377/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2893\n",
            "Epoch [378/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2892\n",
            "Epoch [379/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2893\n",
            "Epoch [380/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2894\n",
            "Epoch [381/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2893\n",
            "Epoch [382/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2892\n",
            "Epoch [383/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2893\n",
            "Epoch [384/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2892\n",
            "Epoch [385/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2894\n",
            "Epoch [386/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2893\n",
            "Epoch [387/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2893\n",
            "Epoch [388/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2891\n",
            "Epoch [389/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2893\n",
            "Epoch [390/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2893\n",
            "Epoch [391/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2893\n",
            "Epoch [392/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2892\n",
            "Epoch [393/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2893\n",
            "Epoch [394/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2893\n",
            "Epoch [395/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2893\n",
            "Epoch [396/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2894\n",
            "Epoch [397/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2892\n",
            "Epoch [398/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2894\n",
            "Epoch [399/2000], Batch [0/4], D_loss: -0.9843, G_loss: 0.2891\n",
            "Epoch [400/2000], Batch [0/4], D_loss: -0.9166, G_loss: 0.2525\n",
            "Epoch [401/2000], Batch [0/4], D_loss: -0.9758, G_loss: 0.2860\n",
            "Epoch [402/2000], Batch [0/4], D_loss: -0.9810, G_loss: 0.2875\n",
            "Epoch [403/2000], Batch [0/4], D_loss: -0.9823, G_loss: 0.2882\n",
            "Epoch [404/2000], Batch [0/4], D_loss: -0.9830, G_loss: 0.2887\n",
            "Epoch [405/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2888\n",
            "Epoch [406/2000], Batch [0/4], D_loss: -0.9834, G_loss: 0.2889\n",
            "Epoch [407/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2890\n",
            "Epoch [408/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2892\n",
            "Epoch [409/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2894\n",
            "Epoch [410/2000], Batch [0/4], D_loss: -0.9843, G_loss: 0.2894\n",
            "Epoch [411/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2893\n",
            "Epoch [412/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2895\n",
            "Epoch [413/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2896\n",
            "Epoch [414/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2895\n",
            "Epoch [415/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2893\n",
            "Epoch [416/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2896\n",
            "Epoch [417/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2895\n",
            "Epoch [418/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2895\n",
            "Epoch [419/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2896\n",
            "Epoch [420/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2896\n",
            "Epoch [421/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2896\n",
            "Epoch [422/2000], Batch [0/4], D_loss: -0.9850, G_loss: 0.2897\n",
            "Epoch [423/2000], Batch [0/4], D_loss: -0.9849, G_loss: 0.2897\n",
            "Epoch [424/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2895\n",
            "Epoch [425/2000], Batch [0/4], D_loss: -0.9849, G_loss: 0.2896\n",
            "Epoch [426/2000], Batch [0/4], D_loss: -0.9849, G_loss: 0.2897\n",
            "Epoch [427/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2895\n",
            "Epoch [428/2000], Batch [0/4], D_loss: -0.9850, G_loss: 0.2897\n",
            "Epoch [429/2000], Batch [0/4], D_loss: -0.9850, G_loss: 0.2897\n",
            "Epoch [430/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2897\n",
            "Epoch [431/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2896\n",
            "Epoch [432/2000], Batch [0/4], D_loss: -0.9851, G_loss: 0.2897\n",
            "Epoch [433/2000], Batch [0/4], D_loss: -0.9563, G_loss: 0.2832\n",
            "Epoch [434/2000], Batch [0/4], D_loss: -0.9810, G_loss: 0.2879\n",
            "Epoch [435/2000], Batch [0/4], D_loss: -0.9820, G_loss: 0.2884\n",
            "Epoch [436/2000], Batch [0/4], D_loss: -0.9823, G_loss: 0.2886\n",
            "Epoch [437/2000], Batch [0/4], D_loss: -0.9828, G_loss: 0.2887\n",
            "Epoch [438/2000], Batch [0/4], D_loss: -0.9832, G_loss: 0.2890\n",
            "Epoch [439/2000], Batch [0/4], D_loss: -0.9837, G_loss: 0.2892\n",
            "Epoch [440/2000], Batch [0/4], D_loss: -0.9837, G_loss: 0.2894\n",
            "Epoch [441/2000], Batch [0/4], D_loss: -0.9839, G_loss: 0.2895\n",
            "Epoch [442/2000], Batch [0/4], D_loss: -0.9841, G_loss: 0.2894\n",
            "Epoch [443/2000], Batch [0/4], D_loss: -0.9842, G_loss: 0.2895\n",
            "Epoch [444/2000], Batch [0/4], D_loss: -0.9845, G_loss: 0.2896\n",
            "Epoch [445/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2896\n",
            "Epoch [446/2000], Batch [0/4], D_loss: -0.9846, G_loss: 0.2898\n",
            "Epoch [447/2000], Batch [0/4], D_loss: -0.9820, G_loss: 0.2887\n",
            "Epoch [448/2000], Batch [0/4], D_loss: -0.9840, G_loss: 0.2890\n",
            "Epoch [449/2000], Batch [0/4], D_loss: -0.9848, G_loss: 0.2899\n",
            "Epoch [450/2000], Batch [0/4], D_loss: -0.9850, G_loss: 0.2900\n",
            "Epoch [451/2000], Batch [0/4], D_loss: -0.9851, G_loss: 0.2900\n",
            "Epoch [452/2000], Batch [0/4], D_loss: -0.9851, G_loss: 0.2901\n",
            "Epoch [453/2000], Batch [0/4], D_loss: -0.9853, G_loss: 0.2901\n",
            "Epoch [454/2000], Batch [0/4], D_loss: -0.9853, G_loss: 0.2901\n",
            "Epoch [455/2000], Batch [0/4], D_loss: -0.9853, G_loss: 0.2902\n",
            "Epoch [456/2000], Batch [0/4], D_loss: -0.9855, G_loss: 0.2902\n",
            "Epoch [457/2000], Batch [0/4], D_loss: -0.9855, G_loss: 0.2902\n",
            "Epoch [458/2000], Batch [0/4], D_loss: -0.9855, G_loss: 0.2902\n",
            "Epoch [459/2000], Batch [0/4], D_loss: -0.9853, G_loss: 0.2902\n",
            "Epoch [460/2000], Batch [0/4], D_loss: -0.9853, G_loss: 0.2901\n",
            "Epoch [461/2000], Batch [0/4], D_loss: -0.9856, G_loss: 0.2902\n",
            "Epoch [462/2000], Batch [0/4], D_loss: -0.9855, G_loss: 0.2903\n",
            "Epoch [463/2000], Batch [0/4], D_loss: -0.9856, G_loss: 0.2902\n",
            "Epoch [464/2000], Batch [0/4], D_loss: -0.9857, G_loss: 0.2903\n",
            "Epoch [465/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2903\n",
            "Epoch [466/2000], Batch [0/4], D_loss: -0.9854, G_loss: 0.2903\n",
            "Epoch [467/2000], Batch [0/4], D_loss: -0.9855, G_loss: 0.2903\n",
            "Epoch [468/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2903\n",
            "Epoch [469/2000], Batch [0/4], D_loss: -0.9857, G_loss: 0.2903\n",
            "Epoch [470/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2904\n",
            "Epoch [471/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2904\n",
            "Epoch [472/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2905\n",
            "Epoch [473/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2904\n",
            "Epoch [474/2000], Batch [0/4], D_loss: -0.9857, G_loss: 0.2904\n",
            "Epoch [475/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2905\n",
            "Epoch [476/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2906\n",
            "Epoch [477/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2905\n",
            "Epoch [478/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2905\n",
            "Epoch [479/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2905\n",
            "Epoch [480/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2904\n",
            "Epoch [481/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2906\n",
            "Epoch [482/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2904\n",
            "Epoch [483/2000], Batch [0/4], D_loss: -0.9856, G_loss: 0.2903\n",
            "Epoch [484/2000], Batch [0/4], D_loss: -0.9856, G_loss: 0.2904\n",
            "Epoch [485/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2905\n",
            "Epoch [486/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2905\n",
            "Epoch [487/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2906\n",
            "Epoch [488/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2905\n",
            "Epoch [489/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2906\n",
            "Epoch [490/2000], Batch [0/4], D_loss: -0.9857, G_loss: 0.2906\n",
            "Epoch [491/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2907\n",
            "Epoch [492/2000], Batch [0/4], D_loss: -0.9861, G_loss: 0.2908\n",
            "Epoch [493/2000], Batch [0/4], D_loss: -0.9861, G_loss: 0.2908\n",
            "Epoch [494/2000], Batch [0/4], D_loss: -0.9862, G_loss: 0.2909\n",
            "Epoch [495/2000], Batch [0/4], D_loss: -0.9861, G_loss: 0.2909\n",
            "Epoch [496/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2908\n",
            "Epoch [497/2000], Batch [0/4], D_loss: -0.9861, G_loss: 0.2908\n",
            "Epoch [498/2000], Batch [0/4], D_loss: -0.9222, G_loss: 0.2677\n",
            "Epoch [499/2000], Batch [0/4], D_loss: -0.9797, G_loss: 0.2883\n",
            "Epoch [500/2000], Batch [0/4], D_loss: -0.9823, G_loss: 0.2890\n",
            "Epoch [501/2000], Batch [0/4], D_loss: -0.9838, G_loss: 0.2900\n",
            "Epoch [502/2000], Batch [0/4], D_loss: -0.9844, G_loss: 0.2901\n",
            "Epoch [503/2000], Batch [0/4], D_loss: -0.9847, G_loss: 0.2902\n",
            "Epoch [504/2000], Batch [0/4], D_loss: -0.9851, G_loss: 0.2902\n",
            "Epoch [505/2000], Batch [0/4], D_loss: -0.9851, G_loss: 0.2904\n",
            "Epoch [506/2000], Batch [0/4], D_loss: -0.9851, G_loss: 0.2903\n",
            "Epoch [507/2000], Batch [0/4], D_loss: -0.9851, G_loss: 0.2905\n",
            "Epoch [508/2000], Batch [0/4], D_loss: -0.9853, G_loss: 0.2905\n",
            "Epoch [509/2000], Batch [0/4], D_loss: -0.9857, G_loss: 0.2905\n",
            "Epoch [510/2000], Batch [0/4], D_loss: -0.9856, G_loss: 0.2906\n",
            "Epoch [511/2000], Batch [0/4], D_loss: -0.9857, G_loss: 0.2907\n",
            "Epoch [512/2000], Batch [0/4], D_loss: -0.9856, G_loss: 0.2907\n",
            "Epoch [513/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2906\n",
            "Epoch [514/2000], Batch [0/4], D_loss: -0.9856, G_loss: 0.2906\n",
            "Epoch [515/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2908\n",
            "Epoch [516/2000], Batch [0/4], D_loss: -0.9857, G_loss: 0.2908\n",
            "Epoch [517/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2907\n",
            "Epoch [518/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2908\n",
            "Epoch [519/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2909\n",
            "Epoch [520/2000], Batch [0/4], D_loss: -0.9859, G_loss: 0.2907\n",
            "Epoch [521/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2909\n",
            "Epoch [522/2000], Batch [0/4], D_loss: -0.9857, G_loss: 0.2908\n",
            "Epoch [523/2000], Batch [0/4], D_loss: -0.9861, G_loss: 0.2908\n",
            "Epoch [524/2000], Batch [0/4], D_loss: -0.9858, G_loss: 0.2907\n",
            "Epoch [525/2000], Batch [0/4], D_loss: -0.9861, G_loss: 0.2909\n",
            "Epoch [526/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2909\n",
            "Epoch [527/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2909\n",
            "Epoch [528/2000], Batch [0/4], D_loss: -0.9862, G_loss: 0.2909\n",
            "Epoch [529/2000], Batch [0/4], D_loss: -0.9863, G_loss: 0.2910\n",
            "Epoch [530/2000], Batch [0/4], D_loss: -0.9861, G_loss: 0.2908\n",
            "Epoch [531/2000], Batch [0/4], D_loss: -0.9861, G_loss: 0.2909\n",
            "Epoch [532/2000], Batch [0/4], D_loss: -0.9860, G_loss: 0.2908\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 2000\n",
        "clip_value = 0.01\n",
        "\n",
        "dataset = CrackDataset(\"/content/drive/MyDrive/LiuLabels\", transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader):\n",
        "        real_images = data.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # Train discriminator\n",
        "        for j in range(5):\n",
        "            D.zero_grad()\n",
        "            z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
        "            fake_images = G(z).detach()\n",
        "            D_real = D(real_images)\n",
        "            D_fake = D(fake_images)\n",
        "            D_loss = D_fake.mean() - D_real.mean()\n",
        "            D_loss.backward()\n",
        "            D_optimizer.step()\n",
        "\n",
        "            for p in D.parameters():\n",
        "                p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "        # Train generator\n",
        "        G.zero_grad()\n",
        "        z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
        "        fake_images = G(z)\n",
        "        D_fake = D(fake_images)\n",
        "        G_loss = -D_fake.mean()\n",
        "        G_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{i}/{len(dataloader)}], D_loss: {D_loss.item():.4f}, G_loss: {G_loss.item():.4f}\")\n",
        "\n",
        "    # Save generated images\n",
        "    with torch.no_grad():\n",
        "        # z = torch.randn(16, 100, 1, 1).to(device)\n",
        "        z = torch.randn(1, 100, 1, 1).to(device)\n",
        "        fake_images = G(z).detach().cpu()\n",
        "        save_image(fake_images, f\"generated_images_{epoch+1}.png\", nrow=4, normalize=True)\n",
        "\n",
        "    # # Save models\n",
        "    # torch.save(G.state_dict(), f\"G_{epoch+1}.pt\")\n",
        "    # torch.save(D.state_dict(), f\"D_{epoch+1}.pt\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDgv4U8/09Q2pOHfR0PeQC",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}